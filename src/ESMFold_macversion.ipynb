{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/20150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 Folding P_13 | Length: 562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20150 [14:44<4950:22:24, 884.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /Users/jianzhouyao/Library/Mobile Documents/com~apple~CloudDocs/Universität/ETH/Python/Digital chemistry/ESM/output/P_13.pdb\n",
      "\n",
      "🔬 Folding P_14 | Length: 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20150 [19:27<6533:12:09, 1167.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenizer([sequence], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 74\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(tokenized)\n\u001b[1;32m     76\u001b[0m pdb \u001b[38;5;241m=\u001b[39m convert_outputs_to_pdb(output)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esmfold.py:2156\u001b[0m, in \u001b[0;36mEsmForProteinFolding.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, masking_pattern, num_recycles)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mesmfold_config\u001b[38;5;241m.\u001b[39membed_aa:\n\u001b[1;32m   2154\u001b[0m     s_s_0 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(masked_aa)\n\u001b[0;32m-> 2156\u001b[0m structure: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk(s_s_0, s_z_0, aa, position_ids, attention_mask, no_recycles\u001b[38;5;241m=\u001b[39mnum_recycles)\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;66;03m# Documenting what we expect:\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m structure \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2159\u001b[0m     k: v\n\u001b[1;32m   2160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m structure\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2171\u001b[0m     ]\n\u001b[1;32m   2172\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esmfold.py:1962\u001b[0m, in \u001b[0;36mEsmFoldingTrunk.forward\u001b[0;34m(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[0m\n\u001b[1;32m   1959\u001b[0m recycle_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecycle_z_norm(recycle_z\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m   1960\u001b[0m recycle_z \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecycle_disto(recycle_bins\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m-> 1962\u001b[0m s_s, s_z \u001b[38;5;241m=\u001b[39m trunk_iter(s_s_0 \u001b[38;5;241m+\u001b[39m recycle_s, s_z_0 \u001b[38;5;241m+\u001b[39m recycle_z, residx, mask)\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;66;03m# === Structure module ===\u001b[39;00m\n\u001b[1;32m   1965\u001b[0m structure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructure_module(\n\u001b[1;32m   1966\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk2sm_s(s_s), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpair\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk2sm_z(s_z)},\n\u001b[1;32m   1967\u001b[0m     true_aa,\n\u001b[1;32m   1968\u001b[0m     mask\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m   1969\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esmfold.py:1946\u001b[0m, in \u001b[0;36mEsmFoldingTrunk.forward.<locals>.trunk_iter\u001b[0;34m(s, z, residx, mask)\u001b[0m\n\u001b[1;32m   1943\u001b[0m z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairwise_positional_embedding(residx, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1946\u001b[0m     s, z \u001b[38;5;241m=\u001b[39m block(s, z, mask\u001b[38;5;241m=\u001b[39mmask, residue_index\u001b[38;5;241m=\u001b[39mresidx, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size)\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s, z\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esmfold.py:1242\u001b[0m, in \u001b[0;36mEsmFoldTriangularSelfAttentionBlock.forward\u001b[0;34m(self, sequence_state, pairwise_state, mask, chunk_size, **_EsmFoldTriangularSelfAttentionBlock__kwargs)\u001b[0m\n\u001b[1;32m   1240\u001b[0m pairwise_state \u001b[38;5;241m=\u001b[39m pairwise_state \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtri_mul_out(pairwise_state, mask\u001b[38;5;241m=\u001b[39mtri_mask))\n\u001b[1;32m   1241\u001b[0m pairwise_state \u001b[38;5;241m=\u001b[39m pairwise_state \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtri_mul_in(pairwise_state, mask\u001b[38;5;241m=\u001b[39mtri_mask))\n\u001b[0;32m-> 1242\u001b[0m pairwise_state \u001b[38;5;241m=\u001b[39m pairwise_state \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow_drop(\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtri_att_start(pairwise_state, mask\u001b[38;5;241m=\u001b[39mtri_mask, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m   1244\u001b[0m )\n\u001b[1;32m   1245\u001b[0m pairwise_state \u001b[38;5;241m=\u001b[39m pairwise_state \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_drop(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtri_att_end(pairwise_state, mask\u001b[38;5;241m=\u001b[39mtri_mask, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m   1247\u001b[0m )\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;66;03m# MLP over pairs.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "# ✅ Set device to MPS on Mac (Metal GPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# ✅ Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\n",
    "    \"facebook/esmfold_v1\", low_cpu_mem_usage=True\n",
    ")\n",
    "model.trunk.set_chunk_size(1)  # ✅ Smaller chunk size for low RAM\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ✅ Function: Convert model output to PDB format\n",
    "\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=(\n",
    "                outputs.get(\"chain_index\", None)[i]\n",
    "                if \"chain_index\" in outputs\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "    return pdbs\n",
    "\n",
    "\n",
    "# ✅ Paths\n",
    "base_output_folder = \"/Users/jianzhouyao/Library/Mobile Documents/com~apple~CloudDocs/Universität/ETH/Python/Digital chemistry/ESM/output\"\n",
    "train_output_folder = os.path.join(base_output_folder, \"train\")\n",
    "test_output_folder = os.path.join(base_output_folder, \"test\")\n",
    "os.makedirs(train_output_folder, exist_ok=True)\n",
    "os.makedirs(test_output_folder, exist_ok=True)\n",
    "\n",
    "train_path = \"/Users/jianzhouyao/Library/Mobile Documents/com~apple~CloudDocs/Universität/ETH/Python/Digital chemistry/ESM/algpred2_train.csv\"\n",
    "test_path = \"/Users/jianzhouyao/Library/Mobile Documents/com~apple~CloudDocs/Universität/ETH/Python/Digital chemistry/ESM/algpred2_test.csv\"\n",
    "\n",
    "# ✅ Load CSVs\n",
    "df_train = pd.read_csv(train_path).dropna(subset=[\"sequence\"])\n",
    "df_test = pd.read_csv(test_path).dropna(subset=[\"sequence\"])\n",
    "\n",
    "# ✅ Fold train sequences\n",
    "for idx, row in tqdm(\n",
    "    df_train.iterrows(), total=len(df_train), desc=\"🔁 Processing TRAIN\"\n",
    "):\n",
    "    pid = str(row[\"id\"])\n",
    "    sequence = row[\"sequence\"].strip()\n",
    "\n",
    "    if not sequence or len(sequence) < 5:\n",
    "        continue\n",
    "\n",
    "    out_file = os.path.join(train_output_folder, f\"{pid}.pdb\")\n",
    "    if os.path.exists(out_file):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n🔬 Folding {pid} | Length: {len(sequence)}\")\n",
    "        tokenized = tokenizer(\n",
    "            [sequence], return_tensors=\"pt\", add_special_tokens=False\n",
    "        )[\"input_ids\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(tokenized)\n",
    "\n",
    "        pdb = convert_outputs_to_pdb(output)\n",
    "\n",
    "        with open(out_file, \"w\") as f:\n",
    "            f.write(\"\".join(pdb))\n",
    "\n",
    "        print(f\"✅ Saved: {out_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed on {pid} | Error: {e}\")\n",
    "\n",
    "# ✅ Fold test sequences\n",
    "for idx, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"🔁 Processing TEST\"):\n",
    "    pid = str(row[\"id\"])\n",
    "    sequence = row[\"sequence\"].strip()\n",
    "\n",
    "    if not sequence or len(sequence) < 5:\n",
    "        continue\n",
    "\n",
    "    out_file = os.path.join(test_output_folder, f\"{pid}.pdb\")\n",
    "    if os.path.exists(out_file):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n🔬 Folding {pid} | Length: {len(sequence)}\")\n",
    "        tokenized = tokenizer(\n",
    "            [sequence], return_tensors=\"pt\", add_special_tokens=False\n",
    "        )[\"input_ids\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(tokenized)\n",
    "\n",
    "        pdb = convert_outputs_to_pdb(output)\n",
    "\n",
    "        with open(out_file, \"w\") as f:\n",
    "            f.write(\"\".join(pdb))\n",
    "\n",
    "        print(f\"✅ Saved: {out_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed on {pid} | Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Sequence length statistics:\n",
      "Train set: min = 50, max = 3326, avg = 354.08\n",
      "Test set:  min = 50, max = 3408, avg = 338.15\n"
     ]
    }
   ],
   "source": [
    "# ✅ Analyze sequence lengths\n",
    "train_lengths = df_train[\"sequence\"].dropna().apply(len)\n",
    "test_lengths = df_test[\"sequence\"].dropna().apply(len)\n",
    "\n",
    "print(\"\\n📊 Sequence length statistics:\")\n",
    "print(\n",
    "    f\"Train set: min = {train_lengths.min()}, max = {train_lengths.max()}, avg = {train_lengths.mean():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set:  min = {test_lengths.min()}, max = {test_lengths.max()}, avg = {test_lengths.mean():.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
