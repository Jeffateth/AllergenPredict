{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the structural features from DSSP with AlgPred 2.0 full protein seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                           sequence  label  Total_SASA  \\\n",
      "0  P_13  MGKPFTLSLSSLCLLLLSSACFAISSSKLNECQLNNLNALEPDHRV...      1   44718.500   \n",
      "1  P_14  MGVFTFEDEINSPVAPATLYKALVTDADNVIPKALDSFKSVENVEG...      1    9042.496   \n",
      "2  P_17  MAEDEDNQQGQGEGLKYLGFVQDAATYAVTTFSNVYLFAKDKSGPL...      1   11549.510   \n",
      "3  P_46  MGVFNYEVETPSVISAARLFKSYVLDGDKLIPKVAPQAITSVENVG...      1    8775.385   \n",
      "4  P_47  MGVFNYEVETPSVIPAARLFKSYVLDGDKLIPKVAPQAITSVENVE...      1    8923.741   \n",
      "\n",
      "   Radius_of_Gyration  Compactness  Contact_Order  SS_Helix  SS_Strand  \\\n",
      "0           33.363918    16.844544       0.056430  0.190391   0.249110   \n",
      "1           15.485167    10.203313       0.126789  0.284810   0.449367   \n",
      "2           21.802948     6.329419       0.044412  0.833333   0.000000   \n",
      "3           15.370365    10.409642       0.130406  0.262500   0.412500   \n",
      "4           15.408467    10.383901       0.127845  0.262500   0.412500   \n",
      "\n",
      "    SS_Coil  \n",
      "0  0.560498  \n",
      "1  0.265823  \n",
      "2  0.166667  \n",
      "3  0.325000  \n",
      "4  0.325000  \n"
     ]
    }
   ],
   "source": [
    "# In one‑hot‑encoding.ipynb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Paths ===\n",
    "DATA_DIR = \"/Users/jianzhouyao/AllergenPredict/data\"\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === 1) Load & merge CSVs ===\n",
    "# Structure\n",
    "structure_train = pd.read_csv(os.path.join(DATA_DIR, \"global_3d_summary_train.csv\"))\n",
    "structure_test = pd.read_csv(os.path.join(DATA_DIR, \"global_3d_summary_test.csv\"))\n",
    "structure_all = pd.concat([structure_train, structure_test], ignore_index=True)\n",
    "# extract id from PDB_File\n",
    "structure_all[\"id\"] = structure_all[\"PDB_File\"].str.replace(\".pdb\", \"\", regex=False)\n",
    "\n",
    "# Sequence\n",
    "seq_train = pd.read_csv(os.path.join(DATA_DIR, \"algpred2_train_seq.csv\"))\n",
    "seq_test = pd.read_csv(os.path.join(DATA_DIR, \"algpred2_test_seq.csv\"))\n",
    "sequence_all = pd.concat([seq_train, seq_test], ignore_index=True)\n",
    "\n",
    "# Merge on 'id' and drop PDB_File\n",
    "merged = pd.merge(sequence_all, structure_all, on=\"id\", how=\"inner\")\n",
    "merged = merged.drop(columns=[\"PDB_File\"])\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "merged.head()\n",
    "\n",
    "# === 2) Train/test split ===\n",
    "train_df, test_df = train_test_split(\n",
    "    merged, test_size=0.2, stratify=merged[\"label\"], random_state=42\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(\"Train / Test shapes:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# === 3) Normalize structure features (fit on train) ===\n",
    "struct_cols = [\n",
    "    \"Total_SASA\",\n",
    "    \"Radius_of_Gyration\",\n",
    "    \"Compactness\",\n",
    "    \"Contact_Order\",\n",
    "    \"SS_Helix\",\n",
    "    \"SS_Strand\",\n",
    "    \"SS_Coil\",\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "X_struct_train = scaler.fit_transform(train_df[struct_cols])\n",
    "X_struct_test = scaler.transform(test_df[struct_cols])\n",
    "\n",
    "# save the scaler\n",
    "with open(os.path.join(OUT_DIR, \"scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# === 4) One-hot encode sequences (common max_len) ===\n",
    "AA_ALPHABET = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AA_ALPHABET)}\n",
    "max_len = max(train_df[\"sequence\"].str.len().max(), test_df[\"sequence\"].str.len().max())\n",
    "\n",
    "\n",
    "def encode_seqs(seq_series):\n",
    "    n = len(seq_series)\n",
    "    X = np.zeros((n, len(AA_ALPHABET), max_len), dtype=np.float32)\n",
    "    for i, seq in enumerate(seq_series):\n",
    "        for pos, aa in enumerate(seq[:max_len]):\n",
    "            idx = AA_TO_IDX.get(aa)\n",
    "            if idx is not None:\n",
    "                X[i, idx, pos] = 1.0\n",
    "    return X\n",
    "\n",
    "\n",
    "X_seq_train = encode_seqs(train_df[\"sequence\"])\n",
    "X_seq_test = encode_seqs(test_df[\"sequence\"])\n",
    "\n",
    "# === 5) Extract labels & ids ===\n",
    "y_train = train_df[\"label\"].values.astype(np.int64)\n",
    "y_test = test_df[\"label\"].values.astype(np.int64)\n",
    "ids_train = train_df[\"id\"].values\n",
    "ids_test = test_df[\"id\"].values\n",
    "\n",
    "# === 6) Save to .npy ===\n",
    "np.save(os.path.join(OUT_DIR, \"X_seq_train.npy\"), X_seq_train)\n",
    "np.save(os.path.join(OUT_DIR, \"X_seq_test.npy\"), X_seq_test)\n",
    "np.save(os.path.join(OUT_DIR, \"X_struct_train.npy\"), X_struct_train)\n",
    "np.save(os.path.join(OUT_DIR, \"X_struct_test.npy\"), X_struct_test)\n",
    "np.save(os.path.join(OUT_DIR, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(OUT_DIR, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(OUT_DIR, \"ids_train.npy\"), ids_train)\n",
    "np.save(os.path.join(OUT_DIR, \"ids_test.npy\"), ids_test)\n",
    "\n",
    "print(\"✔ Preprocessing complete; files written to\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done. Files written to /Users/jianzhouyao/AllergenPredict/data/outputs/\n"
     ]
    }
   ],
   "source": [
    "# In preprocess.ipynb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Paths ===\n",
    "DATA_DIR = \"/Users/jianzhouyao/AllergenPredict/data\"\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === 1) Load merged data ===\n",
    "merged_csv = os.path.join(DATA_DIR, \"merged_allergen_data.csv\")\n",
    "df = pd.read_csv(merged_csv)\n",
    "\n",
    "# === 2) Normalize structure features ===\n",
    "struct_cols = [\n",
    "    \"Total_SASA\",\n",
    "    \"Radius_of_Gyration\",\n",
    "    \"Compactness\",\n",
    "    \"Contact_Order\",\n",
    "    \"SS_Helix\",\n",
    "    \"SS_Strand\",\n",
    "    \"SS_Coil\",\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "df[struct_cols] = scaler.fit_transform(df[struct_cols])\n",
    "\n",
    "# === Save the scaler for later inference ===\n",
    "scaler_path = os.path.join(OUTPUT_DIR, \"scaler.pkl\")\n",
    "with open(scaler_path, \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# === 3) One‑hot‑encode sequences ===\n",
    "AA_ALPHABET = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AA_ALPHABET)}\n",
    "max_len = df[\"sequence\"].str.len().max()\n",
    "\n",
    "# Pre‑allocate array: (n_samples, 20 channels, max_len positions)\n",
    "X_seq = np.zeros((len(df), len(AA_ALPHABET), max_len), dtype=np.float32)\n",
    "for i, seq in enumerate(df[\"sequence\"]):\n",
    "    for pos, aa in enumerate(seq[:max_len]):\n",
    "        idx = AA_TO_IDX.get(aa)\n",
    "        if idx is not None:\n",
    "            X_seq[i, idx, pos] = 1.0\n",
    "\n",
    "# === 4) Extract labels and normalized structural features ===\n",
    "y = df[\"label\"].values.astype(np.int64)\n",
    "X_struct = df[struct_cols].values.astype(np.float32)\n",
    "ids = df[\"id\"].values\n",
    "\n",
    "# === 5) Save everything as .npy for fast PyTorch loading ===\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_seq.npy\"), X_seq)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_struct.npy\"), X_struct)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"y.npy\"), y)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"ids.npy\"), ids)\n",
    "\n",
    "print(f\"Preprocessing done. Files written to {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All outputs present and shapes look good.\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "# 1. confirm files exist\n",
    "for fn in [\n",
    "    \"/Users/jianzhouyao/AllergenPredict/data/outputs/X_seq.npy\",\n",
    "    \"/Users/jianzhouyao/AllergenPredict/data/outputs/X_struct.npy\",\n",
    "    \"/Users/jianzhouyao/AllergenPredict/data/outputs/y.npy\",\n",
    "    \"/Users/jianzhouyao/AllergenPredict/data/outputs/ids.npy\",\n",
    "    \"/Users/jianzhouyao/AllergenPredict/data/outputs/scaler.pkl\",\n",
    "]:\n",
    "    assert os.path.exists(fn), f\"Missing {fn}\"\n",
    "\n",
    "# 2. confirm shapes/types\n",
    "X_seq = np.load(\"/Users/jianzhouyao/AllergenPredict/data/outputs/X_seq.npy\")\n",
    "X_struct = np.load(\"/Users/jianzhouyao/AllergenPredict/data/outputs/X_struct.npy\")\n",
    "y = np.load(\"/Users/jianzhouyao/AllergenPredict/data/outputs/y.npy\")\n",
    "assert X_seq.ndim == 3 and X_seq.dtype == np.float32\n",
    "assert X_struct.ndim == 2 and X_struct.shape[0] == X_seq.shape[0]\n",
    "assert y.ndim == 1 and len(y) == X_seq.shape[0]\n",
    "print(\"✅ All outputs present and shapes look good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allergen-predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
