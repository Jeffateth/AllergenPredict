{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYSIA21I38ElhUhEq/ULN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeffateth/AllergenPredict/blob/main/Decode_EMS_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 1: Install and Load ESM-2"
      ],
      "metadata": {
        "id": "DG8h4mZ8Tb1T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUg29Q12TR2B",
        "outputId": "f3104c02-26d0-4e50-d79b-e4cae2a3aa3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.11/dist-packages (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import esm\n",
        "\n",
        "# Load ESM-2 model (smallest version for demonstration; use larger one if you have GPU resources)\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "# Put model in eval mode (or .train() later if you plan to fine-tune)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsfkvVa4TTZi",
        "outputId": "801dddb7-d3c9-4a4e-aba8-b9dd0c97f68b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ESM2(\n",
              "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
              "  (layers): ModuleList(\n",
              "    (0-5): 6 x TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
              "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (contact_head): ContactPredictionHead(\n",
              "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
              "    (activation): Sigmoid()\n",
              "  )\n",
              "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
              "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 2: Define Attention Pooling and Classifier"
      ],
      "metadata": {
        "id": "dK_JLo6MTej9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, token_embeddings, attention_mask):\n",
        "        # token_embeddings: (batch_size, seq_len, embed_dim)\n",
        "        # attention_mask: (batch_size, seq_len), 1 = real token, 0 = padding\n",
        "        scores = self.attn(token_embeddings).squeeze(-1)  # (batch_size, seq_len)\n",
        "        scores = scores.masked_fill(attention_mask == 0, float('-inf'))  # mask padding\n",
        "        attn_weights = F.softmax(scores, dim=-1)  # (batch_size, seq_len)\n",
        "        weighted_sum = torch.sum(token_embeddings * attn_weights.unsqueeze(-1), dim=1)  # (batch_size, embed_dim)\n",
        "        return weighted_sum, attn_weights  # also return weights for interpretability\n"
      ],
      "metadata": {
        "id": "lyX4x5EWTXw6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Classifier using AttentionPooling"
      ],
      "metadata": {
        "id": "arG_xXpJTlVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinClassifier(nn.Module):\n",
        "    def __init__(self, esm_model, embed_dim, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.esm_model = esm_model  # frozen or fine-tuned\n",
        "        self.pooling = AttentionPooling(embed_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens, attention_mask):\n",
        "        with torch.no_grad():  # optional: freeze ESM-2\n",
        "            results = self.esm_model(tokens, repr_layers=[6])  # use last layer for esm2_t6_8M_UR50D\n",
        "            token_reps = results[\"representations\"][6]  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        pooled, attn_weights = self.pooling(token_reps, attention_mask)\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits, attn_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "pFp08EloTj70"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 3: Preprocessing Sequences and Creating Dataset"
      ],
      "metadata": {
        "id": "eZYr43GETszz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ProteinSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels, batch_converter, max_length=1022):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.batch_converter = batch_converter\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        sequence = self.sequences[idx]\n",
        "        sequence = sequence[:self.max_length]  # truncate if too long\n",
        "        return (f\"protein_{idx}\", sequence), label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch_data, batch_labels = zip(*batch)\n",
        "    labels = torch.tensor(batch_labels, dtype=torch.long)\n",
        "    _, _, tokens = batch_converter(batch_data)  # ESM batch converter handles padding\n",
        "    attention_mask = (tokens != alphabet.padding_idx).long()  # mask: 1=real, 0=pad\n",
        "    return tokens, attention_mask, labels\n"
      ],
      "metadata": {
        "id": "NgCPluMETnYR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 3.2: Load Data for Training"
      ],
      "metadata": {
        "id": "r9By4FMTUThK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(\"algpred2_train.csv\")\n",
        "test_df = pd.read_csv(\"algpred2_test.csv\")\n",
        "\n",
        "# Extract sequences and labels\n",
        "train_sequences = train_df[\"sequence\"].tolist()\n",
        "train_labels = train_df[\"label\"].tolist()\n",
        "\n",
        "test_sequences = test_df[\"sequence\"].tolist()\n",
        "test_labels = test_df[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "rL-STnbrTuiC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 3.3: Create PyTorch Dataloaders"
      ],
      "metadata": {
        "id": "0T9xwekqUmp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test datasets\n",
        "train_dataset = ProteinSequenceDataset(train_sequences, train_labels, batch_converter)\n",
        "test_dataset = ProteinSequenceDataset(test_sequences, test_labels, batch_converter)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "RF6Fa7dYUVvP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 4.1: Initialize the Model"
      ],
      "metadata": {
        "id": "V_fdya9XUvau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "embed_dim = 320  # for esm2_t6_8M_UR50D\n",
        "model = ProteinClassifier(esm_model=model, embed_dim=embed_dim, num_classes=2)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSuPTBDfUphF",
        "outputId": "a7858a9a-5fbd-4e60-e48c-56e30ebed42c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProteinClassifier(\n",
              "  (esm_model): ESM2(\n",
              "    (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (rot_emb): RotaryEmbedding()\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
              "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "        (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (contact_head): ContactPredictionHead(\n",
              "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
              "      (activation): Sigmoid()\n",
              "    )\n",
              "    (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "    (lm_head): RobertaLMHead(\n",
              "      (dense): Linear(in_features=320, out_features=320, bias=True)\n",
              "      (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (pooling): AttentionPooling(\n",
              "    (attn): Linear(in_features=320, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=320, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 4.2: Define Loss and Optimizer"
      ],
      "metadata": {
        "id": "OZYKLXApVDQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "_KIIQ2f5Uyr1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 4.3: Training Loop"
      ],
      "metadata": {
        "id": "ucyrPLLmVGBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for tokens, attention_mask, labels in dataloader:\n",
        "        tokens, attention_mask, labels = tokens.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(tokens, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "lp4oNk9WVFVk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Step 4.4: Training Over Epochs"
      ],
      "metadata": {
        "id": "xgGCzg7aVMWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHpfOPmqVIYc",
        "outputId": "30c3b586-5e4e-4829-e7a8-cab770fcc3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.4166\n",
            "Epoch 2/5, Loss: 0.2615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 5.1: Get Predictions & Attention Scores"
      ],
      "metadata": {
        "id": "sKAR828BXLhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_with_attention(model, dataloader, device, alphabet):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tokens, attention_mask, labels in dataloader:\n",
        "            tokens = tokens.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits, attn_weights = model(tokens, attention_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Convert to CPU for processing\n",
        "            for i in range(tokens.size(0)):\n",
        "                token_ids = tokens[i].cpu().numpy()\n",
        "                sequence = \"\".join([alphabet.get_tok(idx) for idx in token_ids if idx != alphabet.padding_idx])\n",
        "                attention = attn_weights[i][:len(sequence)]  # Trim to real length\n",
        "                label = labels[i].item()\n",
        "                pred = preds[i].item()\n",
        "\n",
        "                results.append((sequence, attention.cpu().numpy(), label, pred))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "XxyAvPvPVLLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 5.2: Visualize Residue-Level Attention (Heatmap)"
      ],
      "metadata": {
        "id": "UuQwglNQXRN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_attention(sequence, attention_weights, title=None):\n",
        "    plt.figure(figsize=(min(len(sequence)*0.25, 20), 2))\n",
        "    sns.heatmap(np.expand_dims(attention_weights, axis=0),\n",
        "                cmap='viridis',\n",
        "                xticklabels=list(sequence),\n",
        "                yticklabels=[],\n",
        "                cbar=True)\n",
        "    plt.title(title or \"Residue Attention\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "SDrUVZ3_XRws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 5.3: Run and Plot for Sample"
      ],
      "metadata": {
        "id": "jN8tbxk2XWK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation and get attention scores\n",
        "results = evaluate_with_attention(model, test_loader, device, alphabet)\n",
        "\n",
        "# Plot the first 3 samples\n",
        "for i in range(3):\n",
        "    sequence, attention, label, pred = results[i]\n",
        "    plot_attention(sequence, attention, title=f\"True: {label}, Pred: {pred}\")\n"
      ],
      "metadata": {
        "id": "T0ib_SsKXSfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxz6NzwfXXcf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}